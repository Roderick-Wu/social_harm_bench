{
  "paperId": "democratic-authoritarian",
  "title": "Democratic or Authoritarian? Probing Political Biases in Large Language Models",
  "modelResults": [
    {
      "modelName": "GPT-4o",
      "organization": "OpenAI",
      "fScaleEnglish": 2.3667,
      "fScaleMandarin": 2.8333,
      "favScoreDemocraticEnglish": 0.1225,
      "favScoreAuthoritarianEnglish": -0.0284,
      "favScoreDemocraticMandarin": 0.0989,
      "favScoreAuthoritarianMandarin": 0.0018,
      "wassersteinDistanceEnglish": 0.1572,
      "wassersteinDistanceMandarin": 0.1015,
      "modelType": "closed-source",
      "downloadable": false,
      "region": "US",
      "benchmarkScores": {
        "f_scale_english": 2.3667,
        "f_scale_mandarin": 2.8333,
        "favscore_democratic_english": 0.1225,
        "favscore_authoritarian_english": -0.0284,
        "favscore_democratic_mandarin": 0.0989,
        "favscore_authoritarian_mandarin": 0.0018,
        "wasserstein_distance_english": 0.1572,
        "wasserstein_distance_mandarin": 0.1015
      }
    },
    {
      "modelName": "Claude-3.7-Sonnet",
      "organization": "Anthropic",
      "fScaleEnglish": 1.8889,
      "fScaleMandarin": 2.1667,
      "favScoreDemocraticEnglish": 0.0549,
      "favScoreAuthoritarianEnglish": -0.0942,
      "favScoreDemocraticMandarin": 0.1991,
      "favScoreAuthoritarianMandarin": 0.0942,
      "wassersteinDistanceEnglish": 0.1506,
      "wassersteinDistanceMandarin": 0.1107,
      "modelType": "closed-source",
      "downloadable": false,
      "region": "US",
      "benchmarkScores": {
        "f_scale_english": 1.8889,
        "f_scale_mandarin": 2.1667,
        "favscore_democratic_english": 0.0549,
        "favscore_authoritarian_english": -0.0942,
        "favscore_democratic_mandarin": 0.1991,
        "favscore_authoritarian_mandarin": 0.0942,
        "wasserstein_distance_english": 0.1506,
        "wasserstein_distance_mandarin": 0.1107
      }
    },
    {
      "modelName": "Llama-4-Maverick",
      "organization": "Meta",
      "fScaleEnglish": 2.7889,
      "fScaleMandarin": 3.8556,
      "favScoreDemocraticEnglish": 0.2082,
      "favScoreAuthoritarianEnglish": 0.0592,
      "favScoreDemocraticMandarin": 0.2243,
      "favScoreAuthoritarianMandarin": 0.1496,
      "wassersteinDistanceEnglish": 0.1490,
      "wassersteinDistanceMandarin": 0.0747,
      "modelType": "open-source",
      "downloadable": true,
      "region": "US",
      "modelSize": "400B",
      "benchmarkScores": {
        "f_scale_english": 2.7889,
        "f_scale_mandarin": 3.8556,
        "favscore_democratic_english": 0.2082,
        "favscore_authoritarian_english": 0.0592,
        "favscore_democratic_mandarin": 0.2243,
        "favscore_authoritarian_mandarin": 0.1496,
        "wasserstein_distance_english": 0.1490,
        "wasserstein_distance_mandarin": 0.0747
      }
    },
    {
      "modelName": "Gemini-2.5-Flash",
      "organization": "Google",
      "fScaleEnglish": 2.0333,
      "fScaleMandarin": 2.2556,
      "favScoreDemocraticEnglish": -0.0058,
      "favScoreAuthoritarianEnglish": -0.1463,
      "favScoreDemocraticMandarin": 0.2054,
      "favScoreAuthoritarianMandarin": 0.0528,
      "wassersteinDistanceEnglish": 0.1434,
      "wassersteinDistanceMandarin": 0.1534,
      "modelType": "closed-source",
      "downloadable": false,
      "region": "US",
      "benchmarkScores": {
        "f_scale_english": 2.0333,
        "f_scale_mandarin": 2.2556,
        "favscore_democratic_english": -0.0058,
        "favscore_authoritarian_english": -0.1463,
        "favscore_democratic_mandarin": 0.2054,
        "favscore_authoritarian_mandarin": 0.0528,
        "wasserstein_distance_english": 0.1434,
        "wasserstein_distance_mandarin": 0.1534
      }
    },
    {
      "modelName": "Grok-3-Beta",
      "organization": "xAI",
      "fScaleEnglish": 2.7333,
      "fScaleMandarin": 2.8778,
      "favScoreDemocraticEnglish": 0.1907,
      "favScoreAuthoritarianEnglish": -0.0461,
      "favScoreDemocraticMandarin": 0.2390,
      "favScoreAuthoritarianMandarin": -0.0084,
      "wassersteinDistanceEnglish": 0.2372,
      "wassersteinDistanceMandarin": 0.2474,
      "modelType": "closed-source",
      "downloadable": false,
      "region": "US",
      "benchmarkScores": {
        "f_scale_english": 2.7333,
        "f_scale_mandarin": 2.8778,
        "favscore_democratic_english": 0.1907,
        "favscore_authoritarian_english": -0.0461,
        "favscore_democratic_mandarin": 0.2390,
        "favscore_authoritarian_mandarin": -0.0084,
        "wasserstein_distance_english": 0.2372,
        "wasserstein_distance_mandarin": 0.2474
      }
    },
    {
      "modelName": "DeepSeek-V3",
      "organization": "DeepSeek",
      "fScaleEnglish": 2.5889,
      "fScaleMandarin": 3.0111,
      "favScoreDemocraticEnglish": 0.2017,
      "favScoreAuthoritarianEnglish": 0.0246,
      "favScoreDemocraticMandarin": 0.2006,
      "favScoreAuthoritarianMandarin": 0.1549,
      "wassersteinDistanceEnglish": 0.1907,
      "wassersteinDistanceMandarin": 0.0582,
      "modelType": "open-source",
      "downloadable": true,
      "region": "Non-US",
      "benchmarkScores": {
        "f_scale_english": 2.5889,
        "f_scale_mandarin": 3.0111,
        "favscore_democratic_english": 0.2017,
        "favscore_authoritarian_english": 0.0246,
        "favscore_democratic_mandarin": 0.2006,
        "favscore_authoritarian_mandarin": 0.1549,
        "wasserstein_distance_english": 0.1907,
        "wasserstein_distance_mandarin": 0.0582
      }
    },
    {
      "modelName": "Qwen3-235B-A22B",
      "organization": "Alibaba",
      "fScaleEnglish": 2.6500,
      "fScaleMandarin": 2.9000,
      "favScoreDemocraticEnglish": 0.1091,
      "favScoreAuthoritarianEnglish": -0.0846,
      "favScoreDemocraticMandarin": 0.2032,
      "favScoreAuthoritarianMandarin": 0.0828,
      "wassersteinDistanceEnglish": 0.1959,
      "wassersteinDistanceMandarin": 0.1336,
      "modelType": "open-source",
      "downloadable": true,
      "region": "Non-US",
      "modelSize": "235B",
      "benchmarkScores": {
        "f_scale_english": 2.6500,
        "f_scale_mandarin": 2.9000,
        "favscore_democratic_english": 0.1091,
        "favscore_authoritarian_english": -0.0846,
        "favscore_democratic_mandarin": 0.2032,
        "favscore_authoritarian_mandarin": 0.0828,
        "wasserstein_distance_english": 0.1959,
        "wasserstein_distance_mandarin": 0.1336
      }
    },
    {
      "modelName": "Ministral-8B",
      "organization": "Mistral",
      "fScaleEnglish": 2.0444,
      "fScaleMandarin": 2.9778,
      "favScoreDemocraticEnglish": -0.0209,
      "favScoreAuthoritarianEnglish": -0.2076,
      "favScoreDemocraticMandarin": 0.3143,
      "favScoreAuthoritarianMandarin": 0.2765,
      "wassersteinDistanceEnglish": 0.1867,
      "wassersteinDistanceMandarin": 0.0380,
      "modelType": "open-source",
      "downloadable": true,
      "region": "Non-US",
      "modelSize": "8B",
      "benchmarkScores": {
        "f_scale_english": 2.0444,
        "f_scale_mandarin": 2.9778,
        "favscore_democratic_english": -0.0209,
        "favscore_authoritarian_english": -0.2076,
        "favscore_democratic_mandarin": 0.3143,
        "favscore_authoritarian_mandarin": 0.2765,
        "wasserstein_distance_english": 0.1867,
        "wasserstein_distance_mandarin": 0.0380
      }
    }
  ]
}
