{
  "siteName": "Jinesis Research Lab",
  "labName": "Jinesis", 
  "version": "2.0",
  "description": "Showcasing cutting-edge research in AI safety, political bias detection, and social harm evaluation from the Jinesis Research Lab.",
  "lastUpdated": "2025-01-15",
  "researchPapers": [
    {
      "id": "socialharmbench",
      "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests", 
      "authors": ["Punya Syon Pandey", "Hai Son Le", "Devansh Bhardwaj", "Zhijing Jin"],
      "year": "2025",
      "url": "https://drive.google.com/file/d/1OxG0CwvK9wiCSbU6TxFwMls69PC_ZaMm/view",
      "datasetUrl": "https://huggingface.co/datasets/psyonp/SocialHarmBench",
      "type": "dataset",
      "description": "A comprehensive evaluation of AI model safety against social harm attacks, measuring attack success rates across various categories of potentially harmful content.",
      "abstract": "Large language models (LLMs) are increasingly deployed in contexts where their failures have the potential to carry sociopolitical consequences. However, existing safety benchmarks sparsely test vulnerabilities in domains such as political manipulation, propaganda generation, or surveillance and information control. To address this gap, we propose SOCIALHARMBENCH, a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries with real-world events, designed to evaluate LLM vulnerabilities to sociopolitical harms.",
      "categories": ["Surveillance & Monitoring", "Censorship & Information Control", "Human Rights Violations", "War Crimes & Crimes Against Humanity", "Historical Revisionism", "Political Manipulation & Election Interference", "Propaganda & Disinformation"],
      "promptCount": 585
    },
    {
      "id": "historical-revisionism",
      "title": "Historical Revisionism Detection in Large Language Models",
      "authors": ["Zhijing Jin", "Research Team"],
      "year": "2025", 
      "url": "https://zhijing-jin.com/files/papers/2025_HistoricalRevisionism.pdf",
      "type": "dataset",
      "description": "Research focusing on detecting and preventing historical revisionism in large language model outputs.",
      "abstract": "This work investigates how large language models handle historical information and their susceptibility to generating or promoting historical revisionism.",
      "categories": ["Historical Accuracy", "Factual Verification", "Content Moderation"],
      "promptCount": 200
    },
    {
      "id": "llm-human-rights",
      "title": "LLM Human Rights: Evaluating AI Alignment with Human Rights Principles",
      "authors": ["Research Team"],
      "year": "2025",
      "url": "https://drive.google.com/file/d/1P0Eb8IAndmNKuwnrZL0fmWiHHRD6Ohd4/view",
      "type": "performance",
      "description": "Comprehensive evaluation of how large language models align with fundamental human rights principles and international law.",
      "abstract": "This study evaluates the alignment of large language models with human rights principles, examining their responses to scenarios involving civil liberties, equality, and justice.",
      "categories": ["Human Rights", "Civil Liberties", "Equality", "Justice"]
    },
    {
      "id": "democratic-authoritarian",
      "title": "Democratic or Authoritarian? Probing Political Biases in Large Language Models",
      "authors": ["David Guzman Piedrahita", "Irene Strauss", "Bernhard Schölkopf", "Rada Mihalcea", "Zhijing Jin"],
      "year": "2025",
      "url": "https://arxiv.org/html/2506.12758v1",
      "type": "performance",
      "description": "Novel methodology to assess LLM alignment with democratic versus authoritarian values through F-scale testing, leader favorability, and role model analysis.",
      "abstract": "As Large Language Models (LLMs) become increasingly integrated into everyday life and information ecosystems, concerns about their implicit biases continue to persist. While prior work has primarily examined socio-demographic and left–right political dimensions, little attention has been paid to how LLMs align with broader geopolitical value systems, particularly the democracy–authoritarianism spectrum.",
      "categories": ["Political Bias", "Democracy", "Authoritarianism", "Value Alignment"]
    }
  ],
  "categories": [
    {
      "id": "sociopolitical-harm",
      "name": "Sociopolitical Harm",
      "description": "Evaluation of model vulnerabilities to sociopolitical manipulation and harmful content generation",
      "promptCount": 585
    },
    {
      "id": "political-bias",
      "name": "Political Bias Detection",
      "description": "Assessment of political biases and value alignment in language models",
      "promptCount": 350
    },
    {
      "id": "human-rights",
      "name": "Human Rights Alignment",
      "description": "Evaluation of model alignment with human rights principles and international law",
      "promptCount": 200
    },
    {
      "id": "historical-accuracy",
      "name": "Historical Accuracy",
      "description": "Testing model accuracy and susceptibility to historical revisionism",
      "promptCount": 200
    }
  ],
  "modelResults": [],
  "samplePrompts": [],
  "sampleResponses": []
}
